{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% prehaces a magic function - matplotlib plots will be produced within the notebook \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import gensim, spacy, logging, warnings\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument \n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    A little boy named Andy loves to be in his roo...\n",
       "1    When two kids find and play a magical board ga...\n",
       "2    Things don't seem to change much in Wabasha Co...\n",
       "3    Hunters and their prey--Neil and his professio...\n",
       "4    An ugly duckling having undergone a remarkable...\n",
       "Name: plot, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing training data\n",
    "df = pd.read_csv('C:/Users/patri/NLP/tagged_plots_movielens.csv')\n",
    "train_file = df['plot']\n",
    "\n",
    "train_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
    "        yield(sent)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing test data\n",
    "df2 = pd.read_csv('C:/Users/patri/NLP/movie_plots_test.csv')\n",
    "test_file = df2['plot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = list(train_file)\n",
    "test_corpus = list(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A little boy named Andy loves to be in his room, playing with his toys, especially his doll named \"Woody\". But, what do the toys do when Andy is not with them, they come to life. Woody believes that he has life (as a toy) good. However, he must worry about Andy\\'s family moving, and what Woody does not know is about Andy\\'s birthday party. Woody does not realize that Andy\\'s mother gave him an action figure known as Buzz Lightyear, who does not believe that he is a toy, and quickly becomes Andy\\'s new favorite toy. Woody, who is now consumed with jealousy, tries to get rid of Buzz. Then, both Woody and Buzz are now lost. They must find a way to get back to Andy before he moves without them, but they will have to pass through a ruthless toy killer, Sid Phillips.', 'When two kids find and play a magical board game, they release a man trapped for decades in it and a host of dangers that can only be stopped by finishing the game.']\n"
     ]
    }
   ],
   "source": [
    "print(train_corpus[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Jules Daly is struggling to raise her orphaned niece and nephew (Maggie and Milo) alone, but it isn't easy after getting downsized out of her antique sales job while Milo rebels against the death of his parents through petty theft. With things looking bleak for Christmas, an English butler named Paisley arrives with an invitation for all to come see the kids' emotionally distant grandfather who lives in Castlebury Hall, somewhere near Liechtenstein. With nothing to hold them back, they go, but the grandfather - Edward, Duke of Castlebury - is rather cold over their visit to his castle. So is his other surviving son, Ashton, Prince of Castlebury. Before long, they're all having a good time and looking forward to hosting a Christmas Eve ball, but Jules overhears a conversation from which she draws a wrong conclusion.\", nan]\n"
     ]
    }
   ],
   "source": [
    "print(test_corpus[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a lemmatizer function - simply returns words to their root form\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-0d825133bc09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtokenized_train_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_corpus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtokenized_train_doc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtokenized_train_doc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "#use list comprehension to create a function that tokenizes and makes each word lowercase\n",
    "tokenized_train_doc = []\n",
    "for d in train_corpus:\n",
    "    tokenized_train_doc.append(word_tokenize(d.lower()))\n",
    "tokenized_train_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-98efa781c137>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtokenized_test_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_corpus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtokenized_test_doc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtokenized_test_doc\u001b[0m\u001b[1;31m#we leave the test document as a list of lists; it should not be converted into gensim format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtokenized_test_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "#we leave the test document as a list of lists; it should not be converted into gensim format\n",
    "tokenized_test_doc = []\n",
    "for d in test_corpus:\n",
    "    tokenized_test_doc.append(word_tokenize(d.lower()))\n",
    "tokenized_test_doc#we leave the test document as a list of lists; it should not be converted into gensim format\n",
    "tokenized_test_doc = []\n",
    "for d in test_corpus:\n",
    "    tokenized_test_doc.append(word_tokenize(d.lower()))\n",
    "tokenized_test_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['adrian', ',', 'on', 'a', 'recent', 'call', 'review', 'you', 'could', 'have', 'asked', 'a', 'better', 'question', 'of', 'the', 'caller', '.', 'this', 'has', 'occurred', 'before', 'where', 'the', 'caller', 'does', 'not', 'quite', 'understand', 'the', 'question', 'leading', 'to', 'longer', 'call', 'times', '.'], tags=[0]),\n",
       " TaggedDocument(words=['adrian', ',', 'your', 'constructive', 'participation', 'in', 'skype', 'on', 'two', 'calls', 'alerted', 'colleagues', 'to', 'situations', 'enabling', 'their', 'efficient', 'handling', 'of', 'the', 'calls', '.'], tags=[1])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert tokenized training document into gensim formated tagged data\n",
    "tagged_train_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_train_doc)]\n",
    "tagged_train_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-03 20:43:47,970 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2020-04-03 20:43:47,987 : INFO : collecting all words and their counts\n",
      "2020-04-03 20:43:47,989 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2020-04-03 20:43:48,246 : INFO : collected 8661 word types and 6545 unique tags from a corpus of 6545 examples and 363678 words\n",
      "2020-04-03 20:43:48,250 : INFO : Loading a fresh vocabulary\n",
      "2020-04-03 20:43:48,273 : INFO : effective_min_count=2 retains 6115 unique words (70% of original 8661, drops 2546)\n",
      "2020-04-03 20:43:48,275 : INFO : effective_min_count=2 leaves 361132 word corpus (99% of original 363678, drops 2546)\n",
      "2020-04-03 20:43:48,325 : INFO : deleting the raw counts dictionary of 8661 items\n",
      "2020-04-03 20:43:48,328 : INFO : sample=0.001 downsamples 49 most-common words\n",
      "2020-04-03 20:43:48,332 : INFO : downsampling leaves estimated 234870 word corpus (65.0% of prior 361132)\n",
      "2020-04-03 20:43:48,367 : INFO : estimated required memory for 6115 words and 10 dimensions: 3808500 bytes\n",
      "2020-04-03 20:43:48,372 : INFO : resetting layer weights\n",
      "2020-04-03 20:43:48,667 : INFO : training model with 4 workers on 6115 vocabulary and 10 features, using sg=0 hs=0 sample=0.001 negative=5 window=2\n",
      "2020-04-03 20:43:49,734 : INFO : EPOCH 1 - PROGRESS: at 91.61% examples, 221189 words/s, in_qsize 3, out_qsize 1\n",
      "2020-04-03 20:43:49,738 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-03 20:43:49,740 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-03 20:43:49,744 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-03 20:43:49,746 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-03 20:43:49,751 : INFO : EPOCH - 1 : training on 363678 raw words (241457 effective words) took 1.0s, 233389 effective words/s\n",
      "2020-04-03 20:43:50,587 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-03 20:43:50,591 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-03 20:43:50,598 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-03 20:43:50,602 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-03 20:43:50,603 : INFO : EPOCH - 2 : training on 363678 raw words (241472 effective words) took 0.8s, 289289 effective words/s\n",
      "2020-04-03 20:43:51,319 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-03 20:43:51,323 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-03 20:43:51,330 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-03 20:43:51,334 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-03 20:43:51,336 : INFO : EPOCH - 3 : training on 363678 raw words (241310 effective words) took 0.7s, 332291 effective words/s\n",
      "2020-04-03 20:43:52,374 : INFO : EPOCH 4 - PROGRESS: at 81.22% examples, 194642 words/s, in_qsize 7, out_qsize 0\n",
      "2020-04-03 20:43:52,535 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-03 20:43:52,541 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-03 20:43:52,557 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-03 20:43:52,561 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-03 20:43:52,564 : INFO : EPOCH - 4 : training on 363678 raw words (241243 effective words) took 1.2s, 198656 effective words/s\n",
      "2020-04-03 20:43:53,629 : INFO : EPOCH 5 - PROGRESS: at 92.30% examples, 213342 words/s, in_qsize 3, out_qsize 1\n",
      "2020-04-03 20:43:53,635 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-03 20:43:53,659 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-03 20:43:53,674 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-03 20:43:53,683 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-03 20:43:53,685 : INFO : EPOCH - 5 : training on 363678 raw words (241082 effective words) took 1.1s, 217466 effective words/s\n",
      "2020-04-03 20:43:53,687 : INFO : training on a 1818390 raw words (1206564 effective words) took 5.0s, 240446 effective words/s\n"
     ]
    }
   ],
   "source": [
    "#Now, we'll instantiate a Doc2Vec model\n",
    "#first parameter is your tags\n",
    "#second parameter,vector size, is the number of results the model will return\n",
    "#the third parameter,window, idk yet \n",
    "#fourth parameter,min_count,is the number of times a word must occur to not be discarded; apparently infrequently occurring words can impact model performance negatively\n",
    "#fifth parameter, workers=cores , use these many worker threads to train the model (=faster training with multicore machines).\n",
    "\n",
    "#Can also pull in the following parameter\n",
    "#If dm=0, distributed bag of words (PV-DBOW) is used; if dm=1,‘distributed memory’ (PV-DM) is used.\n",
    "#min_count=2, ignores all words with total frequency lower than this.\n",
    "#negative=5 , specifies how many “noise words” should be drawn.\n",
    "#hs=0 , and negative is non-zero, negative sampling will be used.\n",
    "#sample=0 , the threshold for configuring which higher-frequency words are randomly down sampled.\n",
    "\n",
    "model = Doc2Vec(tagged_train_data, vector_size=10, window=2, min_count=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00110894  0.02244658  0.02449248 -0.00632843 -0.03135409 -0.00023116\n",
      "  0.01906535 -0.04960615 -0.01973859 -0.01960687]\n"
     ]
    }
   ],
   "source": [
    "#we can use the trained model to infer a vector for any series of tokens based on cosine similarity\n",
    "vector = model.infer_vector(['Lucas', 'you', 'are', 'a', 'bro', 'fires'])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00934602 -0.01839791 -0.02816632  0.02503777 -0.12125958  0.06920887\n",
      "  0.01105353 -0.10474823  0.08355144 -0.08956536]\n"
     ]
    }
   ],
   "source": [
    "vector = model.infer_vector(['This', 'was', 'a', 'great','call','review'])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03782747  0.06630251  0.05836515 -0.02334113  0.01048507  0.00764417\n",
      " -0.0926734  -0.04646827  0.01865775 -0.00746181]\n"
     ]
    }
   ],
   "source": [
    "vector = model.infer_vector(['Thanks', 'for', 'the', 'constructive','feedback'])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we'll first infer new vectors for each document of the training corpus, compare the inferred vectors with the training corpus, and then returning the rank of the document based on self-similarity.\n",
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(tagged_train_data)):\n",
    "    inferred_vector = model.infer_vector(tagged_train_data[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use a counter to evaluate how many times equivalent values are added\n",
    "#I believe these results are saying that the first ranking similarity document is returned most frequently, the second most similar document is returned the second most frequently, which is a good sign \n",
    "import collections\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document (6544): «call review 10/19/19 1229pm . good inbound hotcard call . you were able to get all of the information necessary in order to process this lost card claim .»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d10,n5,w2,mc2,s0.001,t4):\n",
      "\n",
      "MOST (5924, 0.9840618371963501): «call review 9/6/19 4:52pm . keeps conversation in scope , you identified fraud , informed of next steps , and moved on . efficient call .»\n",
      "\n",
      "SECOND-MOST (1723, 0.9803169965744019): «call review 3/16/19 938pm your usual high degree of professionalism came through on this call in terms of efficient and effective process , customer service , respect/courtesy and quality . great work .»\n",
      "\n",
      "MEDIAN (2411, 0.7169733047485352): «call review 4/24/19 6:40pm - this is a call review of an outbound call where you left a message . this message was a bit hurried and rushed . please make sure you are n't rushing through these as it sounds less professional when we do .»\n",
      "\n",
      "LEAST (1309, -0.6040962338447571): «call review 2/21/19 7:27pm . you could not understand the ch ( detection call ) and got an interpreter on the line . good call .»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#similar and dissimilar documents\n",
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(tagged_train_data[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(tagged_train_data[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Document (5094): «call review 8/10/19 1:48pm . manages customer dialogue efficiently . hc call , found the card , blocked it , checked last transaction . good»\n",
      "\n",
      "Similar Document (4058, 0.9087874293327332): «call review 7/1/19 3:18pm . professionally managed the call , manages customer dialogue efficiently . you identified fraud , removed the block , informed of next steps , and moved on . efficient call .»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the corpus and infer a vector from the model\n",
    "import random\n",
    "doc_id = random.randint(0, len(tagged_train_data) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(tagged_train_data[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(tagged_train_data[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (0): «jules daly is struggling to raise her orphaned niece and nephew ( maggie and milo ) alone , but it is n't easy after getting downsized out of her antique sales job while milo rebels against the death of his parents through petty theft . with things looking bleak for christmas , an english butler named paisley arrives with an invitation for all to come see the kids ' emotionally distant grandfather who lives in castlebury hall , somewhere near liechtenstein . with nothing to hold them back , they go , but the grandfather - edward , duke of castlebury - is rather cold over their visit to his castle . so is his other surviving son , ashton , prince of castlebury . before long , they 're all having a good time and looking forward to hosting a christmas eve ball , but jules overhears a conversation from which she draws a wrong conclusion .»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d10,n5,w2,mc2,s0.001,t4):\n",
      "\n",
      "MOST (5276, 0.8755322098731995): «call review 08/25/19 930am . overall good call . just make sure that you are not giving out too much information in related to our services . this particular ch was getting blocked by rt and a tab rule . you explained to the cardholder there was a tab rule rather than a restriction placed by their bank . while it may seem like the same thing , we do n't want to be using tab rule in conversations with our cardholders . let me know if you have questions about this .»\n",
      "\n",
      "MEDIAN (3884, 0.3379250466823578): «call 6/26/2019 4:42:48 pm - on this inbound fwp call the phone number on the account was n't asked for when answering the call . make sure to use the correct scripts .»\n",
      "\n",
      "LEAST (35, -0.7952220439910889): «call review - confirmed fraud - good call , marked appropriate transactions and advised caller to follow up with fi to complete fraud case process .»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(tokenized_test_doc) - 1)\n",
    "inferred_vector = model.infer_vector(tokenized_test_doc[doc_id])\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(tokenized_test_doc[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(tagged_train_data[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
